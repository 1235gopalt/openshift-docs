[[admin-guide-high-availability]]
= High Availability
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
This topic describes how to set up highly-available services on your OpenShift
cluster.

The Kubernetes
xref:../architecture/core_concepts/deployments.adoc#replication-controllers[replication
controller] ensures that the deployment requirements, in particular the number
of replicas, are satisfied when the appropriate resources are available. When
run with two or more replicas, the
xref:../architecture/core_concepts/routes.adoc#routers[router] can be resilient
to failures, providing a highly-available service. Depending on how the router
instances are discovered (via a service, DNS entry, or IP addresses), this could
impose operational requirements to handle failure cases when one or more router
instances are "unreachable".

For some IP-based traffic services, virtual IP addresses (VIPs) should always be
serviced for as long as a single instance is available. This simplifies the
operational overhead and handles failure cases gracefully.

IMPORTANT: Even though
a service is highly available, performance can still be affected.

Use cases for high-availability include:

* I want my cluster to be assigned a resource set and I want the cluster to automatically manage those resources.
* I want my cluster to be assigned a set of VIPs that the cluster manages and migrates (with zero or minimal downtime) on failure conditions, and I should not be required to perform any manual interactions to update the upstream "discovery" sources (e.g., DNS). The cluster should service all the assigned VIPs when at least a single node is available, despite the current available resources not being sufficient to reach the desired state.

You can configure a highly-available router or network setup by running multiple
instances of the pod and fronting them with a balancing tier. This can be
something as simple as DNS round robin, or as complex as multiple load-balancing
layers.
////
=== DNS Round Robin [[dns-round-robin]]

As a simple example, you can create a zone file for a DNS server, such as BIND,
that maps multiple A records for a single domain name. When clients do a lookup,
they are given one of the many records, in order, as a round robin scheme.

[NOTE]
====
The procedure below uses wildcard DNS with multiple A records to achieve the
desired round robin. The wildcard could be further distributed into shards with:

****
`*._<shard>_`
****
====

.To Configure Simple DNS Round Robin:
. Add a new zone that points to your file:
+
====

----
#### named.conf
    zone "v3.rhcloud.com" IN {
            type master;
            file "v3.rhcloud.com.zone";
    };

----
====

. Define the round robin mappings for the DNS lookup:
+
====

----
#### v3.rhcloud.com.zone
    $ORIGIN v3.rhcloud.com.

    @       IN      SOA     . v3.rhcloud.com. (
                         2009092001         ; Serial
                             604800         ; Refresh
                              86400         ; Retry
                            1206900         ; Expire
                                300 )       ; Negative Cache TTL
            IN      NS      ns1.v3.rhcloud.com.
    ns1     IN      A       127.0.0.1
    *       IN      A       10.245.2.2
            IN      A       10.245.2.3


----
====

. Test the entry. The following example test uses `dig` (available in the
*bind-utils* package) in a *Vagrant* environment to show multiple answers for
the same lookup. Performing multiple pings shows the resolution swapping between
IP addresses:
+
[options="nowrap"]
====

----

$ dig hello-openshift.shard1.v3.rhcloud.com

; <<>> DiG 9.9.4-P2-RedHat-9.9.4-16.P2.fc20 <<>> hello-openshift.shard1.v3.rhcloud.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 36389
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;hello-openshift.shard1.v3.rhcloud.com. IN A

;; ANSWER SECTION:
hello-openshift.shard1.v3.rhcloud.com. 300 IN A	10.245.2.2
hello-openshift.shard1.v3.rhcloud.com. 300 IN A	10.245.2.3

;; AUTHORITY SECTION:
v3.rhcloud.com.		300	IN	NS	ns1.v3.rhcloud.com.

;; ADDITIONAL SECTION:
ns1.v3.rhcloud.com.	300	IN	A	127.0.0.1

;; Query time: 5 msec
;; SERVER: 10.245.2.3#53(10.245.2.3)
;; WHEN: Wed Nov 19 19:01:32 UTC 2014
;; MSG SIZE  rcvd: 132

$ ping hello-openshift.shard1.v3.rhcloud.com
PING hello-openshift.shard1.v3.rhcloud.com (10.245.2.3) 56(84) bytes of data.
...
^C
--- hello-openshift.shard1.v3.rhcloud.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.272/0.573/0.874/0.301 ms

$ ping hello-openshift.shard1.v3.rhcloud.com
[...]
----

====
////

[[configuring-ip-failover]]
== Configuring IP Failover

Using IP failover involves switching IP addresses to a redundant or stand-by
set of nodes on failure conditions.

The `oadm ipfailover` command helps set up the VIP failover configuration. As
an administrator, you can configure IP failover on an entire cluster, or on a
subset of nodes, as defined by the labeled selector. If you are running in
production, match the labeled selector with at least two nodes to ensure you
have failover protection and provide a `--replicas=<n>` value that matches the
number of nodes for the given labeled selector:

----
$ oadm ipfailover [<Ip_failover_config_name>] <options> --replicas=<n>
----

////
You can view what the configuration configuration that would look like
using one of the supported formats (the example below uses the JSON format):

----
$ oadm ipfailover [<Ip_failover_config_name>] <options> -o json
----

==== ipfailover command options (subset)
The list of command options described here are a subset that are relevant to this document.

            <options> = One or more of:
                --create
                -l,--selector=<selector>
                --virtual-ips=<ip-range>
                -i|--interface=<interface>
                -w|--watch-port=<port>

            <credentials> = <string> - Path to .kubeconfig file containing credentials to use to contact the master.
            <selector> = <string> - The node selector to use for running the HA sidecar pods.
            <ip-range> = string - One or more comma separated IP address or ranges.
                                  Example: 10.2.3.42,10.2.3.80-84,10.2.3.21
            <interface> = <string> - The interface to use.
                                     Default: Default interface on node or eth0
            <port> = <number> - Port to watch for resource availability.
                                Default: 80.
            <string> = a string of characters.
            <number> = a number ([0-9]*).
////

The `oadm ipfailover` command ensures that a failover pod runs on each of
the nodes matching the constraints or label used. This pod uses VRRP (Virtual
Router Redundancy Protocol) with link:http://www.keepalived.org/[Keepalived] to ensure that the service on the
watched port is available, and, if needed, Keepalived will automatically float
the VIPs if the service is not available.

[[configuring-a-highly-available-routing-service]]
=== Configuring a Highly-available Routing Service
The following steps describe how to set up a highly-available router environment
with IP failover:

*Keepalived* on each node determines whether the needed service is running. If it is, VIPs are supported and *Keepalived* participates in the negotiation to determine which node will serve the VIP. For a node to participate, the service must be listening on the watch port on a VIP or the check must be disabled.

[NOTE]
====
Each VIP in the set may end up being served by a different node.
====

[[check-notify]]
=== Check and Notify Scripts

The *keepalived* port monitoring feature supports one or two scripts that are
run on a configured interval to verify that the application is available. The
default script uses a simple
xref:../install_config/install/prerequisites.adoc#required-ports[TCP
connection] to verify that the application is running. This test is suppressed
when the monitoring port is set to `0`. The administrator can supply an
additional script that does the needed verification. For example, the script can
test a web server by issuing a request and verifying the response. The script
must exit with `0` for *PASS* and `1` for *FAIL*.

The administrator provides the additional script, via the
`--check-script=<script>` option. By default, the check is done every two
seconds, but can be changed using the `--check-interval=<seconds>` option.

For each VIP, *keepalived* keeps the state of the node. The VIP on the node may
be in *MASTER*, *BACKUP*, or *FAULT* state. All VIPs on the node that are not in
the *FAULT* state participate in the negotiation to decide which will be
*MASTER* for the VIP. All of the losers enter the *BACKUP* state. When the
*check* script on the *MASTER* fails, the VIP enters the *FAULT* state and
triggers a renegotiation. When the *BACKUP* fails, the VIP enters the *FAULT*
state. When the *check* script passes again on a VIP in the *FAULT* state, it
exits *FAULT* and negotiates for *MASTER*. The resulting state is either
*MASTER* or *BACKUP*.

The administrator can provide an optional *notify* script, which is called
whenever the state changes. *Keepalived* passes the following three parameters
to the script:

* `$1` - "GROUP"|"INSTANCE"
* `$2` - Name of the group or instance
* `$3` - The new state ("MASTER"|"BACKUP"|"FAULT")

These scripts run in the IP failover pod and use the pod's file system, not the
host file system. The options require the full path to the script. The
administrator must make the script available in the pod to extract the results
from running the *notify* script. The recommended approach for providing the
scripts is to use a
xref:../dev_guide/configmaps.adoc#dev-guide-configmaps[ConfigMap].

The full path names of the *check* and *notify* scripts are added to the
*keepalived* configuration file, *_ /etc/keepalived/keepalived.conf_*, which is
loaded every time *keepalived* starts. The scripts can be added to the pod with
a ConfigMap as follows.

. Create the desired script and create a ConfigMap to hold it. The script
has no input arguments and must return `0` for *OK* and `1` for *FAIL*.
+
The check script, *_mycheckscript.sh_*:
+
The following example defines a label as router instances that are servicing
traffic in the US west geography *ha-router=geo-us-west*:
+
====
----
$ oc set env dc/ipf-ha-router \
    OPENSHIFT_HA_CHECK_SCRIPT=/etc/keepalive/mycheckscript.sh
$ oc volume dc/ipf-ha-router --add --overwrite \
    --name=config-volume \
    --mount-path=/etc/keepalive \
    --source='{"configMap": { "name": "mycustomcheck"}}'
----
====

Before starting the *keepalived* daemon, the startup script verifies the
`iptables` rule that allows multicast traffic to flow.  If there is no such
rule, the startup script creates a new rule and adds it to the IP tables
configuration.  Where this new rule gets added to the IP tables configuration
depends on the `--iptables-chain=` option. If there is an `--iptables-chain=`
option specified, the rule gets added to the specified chain in the option.
Otherwise, the rule is added to the `INPUT` chain.


[IMPORTANT]
====
The `iptables` rule must be present whenever there is one or more *keepalived* daemon running on the node.
====

The `iptables` rule can be removed after the last *keepalived* daemon terminates. The rule is not automatically removed.

You can manually manage the `iptables` rule on each of the nodes. It only gets created when none is present (as long as ipfailover is not created with the -`-iptable-chain=""` option).

[IMPORTANT]
====
You must ensure that the manually added rules persist after a system restart.

Be careful since every *keepalived* daemon uses the VRRP protocol over multicast 224.0.0.18 to negotiate with its peers.  There must be a different VRRP-id (in the range 0..255) for
xref:../admin_guide/high_availability.adoc#ha-vrrp-id-offset[each VIP].
====
----
$ for node in openshift-node-{5,6,7,8,9}; do   ssh $node <<EOF

export interface=${interface:-"eth0"}
echo "Check multicast enabled ... ";
ifconfig $interface | grep -i MULTICAST

echo "Check multicast groups ... "
netstat -g -n | grep 224.0.0 | grep $interface

echo "Optionally, add accept rule and persist it ... "
sudo /sbin/iptables -I INPUT -i $interface -d 224.0.0.18/32 -j ACCEPT

echo "Please ensure the above rule is added on system restarts."

EOF
done;
----
====

. Depending on your environment policies, you can either reuse the *router*
service account created previously or create a new *ipfailover* service account.
+
Ensure that either the *router* service account exists as described in
xref:../install_config/router/index.adoc#install-config-router-overview[Deploying a Router] or create
a new *ipfailover* service account. The example below creates a new service
account with the name *ipfailover* in the *default* namespace:
+
====
----
$ oc create serviceaccount ipfailover -n default
----
====

. Add the *ipfailover* service account in the *default* namespace to the *privileged* SCC:
+
====
----
$ oadm policy add-scc-to-user privileged system:serviceaccount:default:ipfailover
----
====

. Start the router with at least two replicas on nodes matching the labels used
in the first step. The following example runs three instances using the
*ipfailover* service account:
+
ifdef::openshift-enterprise[]
====
----
$ oadm router ha-router-us-west --replicas=5 \
    --selector="ha-svc-nodes=geo-us-west" \
    --labels="ha-svc-nodes=geo-us-west" \
    --service-account=ipfailover
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router ha-router-us-west --replicas=5 \
    --selector="ha-svc-nodes=geo-us-west" \
    --labels="ha-svc-nodes=geo-us-west" \
    --service-account=ipfailover
----
====
endif::[]
+
[NOTE]
====
The above command runs fewer router replicas than available nodes, so
that, in the chance of node failures, Kubernetes can still ensure three
available instances until the number of available nodes labeled
*ha-router=geo-us-west* is below three. Additionally, the router uses the host
network as well as ports 80 and 443, so fewer number of replicas are running to
ensure a higher Service Level Availability (SLA). If there are no constraints on
the service being setup for failover, it is possible to target the service to
run on one or more, or even all, of the labeled nodes.
====

. Finally, configure the VIPs and failover for the nodes labeled with
*ha-router=geo-us-west* in the first step. Ensure the number of replicas match
the number of nodes and that they satisfy the label setup in the first step. The
name of the *ipfailover* configuration (*ipf-ha-router-us-west* in the example
below) should be different from the name of the router configuration
(*ha-router-us-west*) as both the router and *ipfailover* create deployment
configuration with those names. Specify the VIPs addresses and the port number
that *ipfailover* should monitor on the desired instances:
+
ifdef::openshift-enterprise[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --iptables-chain="INPUT" \
    --service-account=ipfailover --create
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --iptables-chain="INPUT" \
    --service-account=ipfailover --create
----
====
endif::[]

For details on how to dynamically update the virtual IP addresses for high
availability, see
xref:dynamically-updating-vips-for-a-highly-available-service[Dynamically
Updating Virtual IPs for a Highly-available Service].

=== Configuring a Highly-available Network Service [[ip-failover]]

The following steps describe how to set up a highly-available IP-based network
service with IP failover:

. Label the nodes for the service. This step can be optional if you run the
service on any of the nodes in your Kubernetes cluster and use VIPs that can
float within those nodes. This process may already exist within a complex
cluster, in that the nodes may be filtered by any constraints or requirements
specified (e.g., nodes with SSD drives, or higher CPU, memory, or disk
requirements, etc.).
+
The following example labels a highly-available cache service that is listening
on port 9736 as *ha-cache=geo*:
+
Below is ipfailover command for the *geo-cache* service that is listening on
port 9736.  Since there are two ipfailover deployment configration the
`--vrrp-id-offset` must be set, so that each VIP gets its own offset. In this
case, setting a value of 10 means that the `ipf-ha-router-us-west` can have a
maximum of 10 VIPs (0-9) since `ipf-ha-geo-cache` is starting at 10.
+
ifdef::openshift-enterprise[]
====
----
$ oadm ipfailover ipf-ha-geo-cache \
    --replicas=5 --watch-port=9736 \
    --selector="ha-svc-nodes=geo-us-west" \
    --virtual-ips=10.245.3.101-105 \
    --vrrp-id-offset=10 \
    --service-account=ipfailover --create
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm ipfailover ipf-ha-geo-cache \
    --replicas=4 --selector="ha-cache=geo" \
    --virtual-ips=10.245.2.101-104 --watch-port=9736 \
    --replicas=5 --watch-port=9736 \
    --selector="ha-svc-nodes=geo-us-west" \
    --virtual-ips=10.245.3.101-105 \
    --vrrp-id-offset=10 \
    --service-account=ipfailover --create
----
====
endif::[]
////
+
As an alternative, the following example creates an IP failover configuration on
a selection of nodes labeled "my-ha-service=har-reporter" (on 4 nodes with 7
VIPs monitoring a service listening on port 4242:
+
The administrator sets up external DNS to point to the VIP addresses knowing that all the *router* VIPs point to the same *router*, and all the *geo-cache* VIPs point to the same *geo-cache* service. As long as one node
remains running, all the VIP addresses are served.

. Deploy the ipfailover router to monitor postgresql listening on node
port 32439 and the external IP address, as defined in the *postgresql-ingress*
service:
+
====
----
$ oadm ipfailover ipf-ha-postgresql \
    --replicas=1 <1> --selector="app-type=postgresql" <2> \
    --virtual-ips=10.9.54.100 <3> --watch-port=32439 <4>  \
    --credentials=/etc/origin/master/openshift-router.kubeconfig \
    --service-account=ipfailover --create
----
<1> Specifies the number of instances to deploy.
<2> Restricts where the ipfailover is deployed.
<3> Virtual IP address to monitor.
<4> Port on which ipfailover will monitor on each node.
====

[[dynamically-updating-vips-for-a-highly-available-service]]
=== Dynamically Updating Virtual IPs for a Highly-available Service

The default deployment strategy for the IP failover service is to recreate
the deployment. In order to dynamically update the virtual IPs for a highly
available routing service with minimal or no downtime, you must:

- update the IP failover service deployment configuration to use a rolling update
strategy, and
- update the `*OPENSHIFT_HA_VIRTUAL_IPS*` environment variable with the updated
list or sets of virtual IP addresses.

The following example shows how to dynamically update the deployment strategy
and the virtual IP addresses:

. Consider an IP failover configuration that was created using the following:
+
ifdef::openshift-enterprise[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --service-account=ipfailover --create
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --service-account=ipfailover --create
----
====
endif::[]

. Edit the deployment configuration:
+
====
----
$ oc edit dc/ipf-ha-router-us-west
----
====

. Update the `*spec.strategy.type*` field from `Recreate` to `Rolling`:
+
====
----
spec:
  replicas: 5
  selector:
    ha-router: geo-us-west
  strategy:
    recreateParams:
      timeoutSeconds: 600
    resources: {}
    type: Rolling <1>
----
<1> Set to `Rolling`.
====

. Update the `*OPENSHIFT_HA_VIRTUAL_IPS*` environment variable to contain the
additional virtual IP addresses:
+
====
----
- name: OPENSHIFT_HA_VIRTUAL_IPS
  value: 10.245.2.101-105,10.245.2.110,10.245.2.201-205 <1>
----
<1> `10.245.2.110,10.245.2.201-205` have been added to the list.
====
