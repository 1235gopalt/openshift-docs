////
Removing an etcd host

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* admin_guide/assembly_restore-etcd-quorum.adoc
////

If an etcd host fails beyond restoration, remove it from the cluster.
ifeval::["{context}" == "restore-etcd-quorum"]
To recover from an etcd quorum loss, you must also remove all healthy etcd
nodes but one from your cluster.
endif::[]

ifeval::["{context}" == "day_two_host_level_tasks"]
[IMPORTANT]
====
Ensure the etcd cluster maintains quorum while removing the etcd host by
removing a single host at a time from a cluster.
====
endif::[]

*Steps to be performed on all masters hosts*

[discrete]
=== Procedure

. Remove each other etcd host from the etcd cluster. Run the following command
for each etcd node:
+
----
# etcdctl -C https://<surviving host IP address>:2379 \
  --ca-file=/etc/etcd/ca.crt     \
  --cert-file=/etc/etcd/peer.crt     \
  --key-file=/etc/etcd/peer.key member remove <failed member ID>
----

ifeval::["{context}" == "day_two_host_level_tasks"]
. Edit the failed etcd host out of the `/etc/origin/master/master-config.yaml`
master configuration file on every master:
+
----
etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key
  urls:
    - https://master-0.example.com:2379
    - https://master-1.example.com:2379
    - https://master-2.example.com:2379 <1>
----
endif::[]
ifeval::["{context}" == "restore-etcd-quorum"]
. Remove the other etcd hosts from the `/etc/origin/master/master-config.yaml`
+master configuration file on every master:
+
----
etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key
  urls:
    - https://master-0.example.com:2379
    - https://master-1.example.com:2379 <1>
    - https://master-2.example.com:2379 <1>
----
endif::[]
<1> The host to remove.

. Restart the master API service on every master:
+
----
# systemctl restart atomic-openshift-master-api
----
+
Or, if using a single master cluster installation: 
+
----
# systemctl restart atomic-openshift-master
----

*Steps to be performed in the current etcd cluster*

[discrete]
=== Procedure

. Remove the failed host from the cluster:
+
----
# etcdctl2 cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
failed to check the health of member 8372784203e11288 on https://192.168.55.21:2379: Get https://192.168.55.21:2379/health: dial tcp 192.168.55.21:2379: getsockopt: connection refused
member 8372784203e11288 is unreachable: [https://192.168.55.21:2379] are all unreachable
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy

# etcdctl2 member remove 8372784203e11288 <1>
Removed member 8372784203e11288 from cluster

# etcdctl2 cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
<1> The `remove` command requires the etcd ID, not the hostname.

. To ensure the etcd configuration does not use the failed host when the etcd
service is restarted, modify the `/etc/etcd/etcd.conf` file on all remaining
etcd hosts and remove the failed host in the value for the
`ETCD_INITIAL_CLUSTER` variable:
+
----
# vi /etc/etcd/etcd.conf
----
+
For example:
+
----
ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380,master-2.example.com=https://192.168.55.13:2380
----
+
becomes:
+
----
ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380
----
+
[NOTE]
====
Restarting the etcd services is not required, because the failed host is
removed using `etcdctl`.
====

. Modify the Ansible inventory file to reflect the current status of the cluster
and to avoid issues when re-running a playbook:
+
----
[OSEv3:children]
masters
nodes
etcd

... [OUTPUT ABBREVIATED] ...

[etcd]
master-0.example.com
master-1.example.com
----

. If you are using Flannel, modify the `flanneld` service configuration located
at `/etc/sysconfig/flanneld` on every host and remove the etcd host:
+
----
FLANNEL_ETCD_ENDPOINTS=https://master-0.example.com:2379,https://master-1.example.com:2379,https://master-2.example.com:2379
----

. Restart the `flanneld` service:
+
----
# systemctl restart flanneld.service
----

ifeval::["{context}" == "day_two_host_level_tasks"]
== Replacing an etcd host

Before you remove the etcd host, scale up the etcd cluster with the new host
using the scale up Ansible playbook or
xref:../day_two_guide/host_level_tasks.adoc#scaling-etcd[the manual procedure in
Scaling etcd]. This ensures that you keep quorum if you lose an etcd host during
the replacement procedure. Then, you can xref:../day_two_guide/host_level_tasks.adoc#removing-an-etcd-host[remove the etcd host from the cluster].

[WARNING]
====
The etcd cluster must maintain a quorum during the replacement operation. This
means that at least one host must be in operation at all times.

If the host replacement operation occurs while the etcd cluster maintains a
quorum, cluster operations are not affected, except if there is a large etcd
data to replicate where some operations can be slowed down.
====

[NOTE]
====
Ensure a backup of etcd data and configuration files exists before any procedure
involving the etcd cluster to ensure restoration in the case of failure.
====
endif::[] 
