[[install-config-persistent-storage-persistent-storage-ceph-rbd]]
= Persistent Storage Using Ceph Rados Block Device (RBD)
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
OpenShift can utilize persistent storage using Block Storage
like Ceph RBD. Some familiarity with Kubernetes and Docker is assumed. It is
also assumed that there is access to an existing Ceph cluster and volume,
and that *ceph-fuse* has been installed on all OpenShift nodes in the
cluster.

The Kubernetes
link:../../architecture/additional_concepts/storage.html[persistent volume]
framework allows administrators to provision a cluster with persistent storage
and gives users a way to request those resources without having any knowledge of
the underlying infrastructure. Persistent volumes are not bound to a single
project or namespace; they can be shared across the OpenShift cluster.
link:../../architecture/additional_concepts/storage.html#persistent-volume-claims[Persistent
volume claims], however, are specific to a project or namespace and can be
requested by users.

[IMPORTANT]
====
High-availability of storage in the infrastructure is left to the underlying
storage provider.
====

[[ceph-provisioning]]

== Provisioning
Storage must exist in the underlying infrastructure before it can be mounted as
a volume in OpenShift. To provision Ceph volumes in OpenShift, the
following are required:

- Existing Ceph volumes, to be defined in the persistent volume object
- The `*PersistentVolume*` API

You must also ensure *ceph-fuse* is installed on all OpenShift nodes in the cluster:

----
# yum install ceph ceph-common
----


[[ceph-creating-persistent-volume]]

=== Creating the Persistent Volume

You must define your persistent volume in an object definition before creating
it in OpenShift:

.Persistent Volume Object Definition Using Ceph RBD
====

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: "ceph-pv" <1>
spec:
  capacity:
    storage: "2Gi" <2>
  accessModes:
    - "ReadWriteOnce" <3>
  rbd: <4>
    monitors:
      - "192.168.122.111:6789" <5>
    pool: rbd
    image: foo
    user: admin
    secretRef:
      name: "ceph-secret" <6>
    fsType: ext4
    readOnly: false
  persistentVolumeReclaimPolicy: "Recycle"
----
<1> The name of the volume. This will be how it is identified via
link:../../architecture/additional_concepts/storage.html[persistent volume
claims] or from pods.
<2> The amount of storage allocated to this volume.
<3> This is the access type to the block storage. All block storage is defined to be single user (non-shared storage).
<4> This defines the volume type being used, in this case the *rbd*
plug-in.
<5> This is the Ceph monitor address and port.
<6> This is the Ceph secret used to create a secure connection from OSE to the Ceph server. This must be configured ahead of time
as part of the Ceph configuration.
====

Save your definition to a file, for example *_ceph-pv.yaml_*, and create the
persistent volume:

====
----
# oc create -f ceph-pv.yaml
persistentvolume "ceph-pv" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
ceph-pv                  <none>    2147483648   RWO           Available                       2s
----
====

Users can then link:../../dev_guide/persistent_volumes.html[request storage
using persistent volume claims], which can now utilize your new persistent
volume.

[IMPORTANT]
====
Persistent volume claims only exist in the user's namespace and can only be
referenced by a pod within that same namespace. Any attempt to access a
persistent volume from a different namespace causes the pod to fail.
====
