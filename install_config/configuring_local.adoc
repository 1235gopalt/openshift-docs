[[install-config-configuring-local]]
= Configuring for Local Volume
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access
xref:../install_config/persistent_storage/persistent_storage_local.adoc#install-config-persistent-storage-persistent-storage-local[local
volumes] for application data. Local volumes are persistent volumes (PVs) that
represent locally-mounted file systems. In the future, they may be extended to
raw block devices.

Local volumes are different from hostPath. They have a special annotation that
makes any pod that uses the PV to be scheduled on the same node where the local
volume is mounted.

In addition, local volumes include a provisioner that automatically creates PVs
for locally mounted devices. This provisioner is currently limited and it only
scans pre-configured directories. It cannot dynamically provision volumes, which
may be implemented in a future release.

The local volume provisioner allows using local storage within {product-title}
and supports:

* Volumes
* PVs

[NOTE]
====
Local volumes is an alpha feature and may change in a future release of {product-title}.
====

[[local-volume-enabling-local-volumes]]
=== Enable Local Volumes
Enable the `PersistentLocalVolumes` feature gate on all masters and nodes:

. Edit or create the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and add
`PersistentLocalVolumes=true` under the `apiServerArguments` and
`controllerArguments` sections:
+
[source, yaml]
----
apiServerArguments:
   feature-gates:
   - PersistentLocalVolumes=true
...

 controllerArguments:
   feature-gates:
   - PersistentLocalVolumes=true
...
----

. On all nodes, edit or create the node configuration file
(*_/etc/origin/node/node-config.yaml_* by default) and add
`PersistentLocalVolumes=true` feature gate under `kubeletArguments`.
+
[source, yaml]
----
kubeletArguments:
   feature-gates:
     - PersistentLocalVolumes=true
----

The `LocalPersistentVolumes` alpha feature also requires the `VolumeScheduling`
alpha feature. This is a breaking change, and the following changes are
required: 

* The `VolumeScheduling` feature gate must also be enabled on kube-scheduler and kube-controller-manager components.
* The `NoVolumeNodeConflict` predicate has been removed. For non-default schedulers, update your scheduler policy.
* The `CheckVolumeBinding` predicate must be enabled in non-default schedulers.

See https://kubernetes.io/docs/reference/feature-gates/[Feature Gates] for an
overview of the various feature gates an administrator can specify on different
Kubernetes components.

[[local-volume-mounting-local-volumes]]
=== Mount Local Volumes
All local volumes must be manually mounted before they can be consumed by {product-title} as PVs.

All volumes must be mounted into the
*_/mnt/local-storage/<storage-class-name>/<volume>_* path. Administrators are required to create the local devices as needed (by using any method such as
a disk partition or an LVM), create suitable file systems on these devices, and mount them using a script or `/etc/fstab` entries.

.Example `/etc/fstab` entries:

[source]
----
# device name   # mount point                  # FS    # options # extra
/dev/sdb1       /mnt/local-storage/ssd/disk1 ext4     defaults 1 2
/dev/sdb2       /mnt/local-storage/ssd/disk2 ext4     defaults 1 2
/dev/sdb3       /mnt/local-storage/ssd/disk3 ext4     defaults 1 2
/dev/sdc1       /mnt/local-storage/hdd/disk1 ext4     defaults 1 2
/dev/sdc2       /mnt/local-storage/hdd/disk2 ext4     defaults 1 2
----

All volumes must be accessible to processes running within Docker containers. Change the labels of mounted filesystems to allow that:

[source, bash]
---
$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /mnt/local-storage/
---

[[local-volume-configure-local-provisioner]]
=== Configure Local Provisioner
{product-title} depends on an external provisioner to create PVs for local devices and to clean them up when they are not needed (to enable reuse).

[NOTE]
====
* The local volume provisioner is different from most provisioners and does not support dynamic provisioning.
* The local volume provisioner requires that administrators preconfigure local volumes on each node and mount them under discovery directories. The provisioner then manages the volumes by creating and cleaning up PVs for each volume.
====

This external provisioner needs to be configured using a ConfigMap to relate
directories with storage classes. ConfigMaps allow you to decouple configuration
artifacts from image content to keep containerized applications portable. This
configuration must be created before the provisioner is deployed.

[NOTE]
====
_(Optional)_ Create a standalone namespace for local volume provisioner and its configuration, for example:
`oc new-project local-storage`
====

[source, yaml]
----
kind: ConfigMap
metadata:
  name: local-volume-config
data:
    "local-ssd": | <1>
      {
        "hostDir": "/mnt/local-storage/ssd", <2>
        "mountDir": "/mnt/local-storage/ssd" <3>
      }
    "local-hdd": |
      {
        "hostDir": "/mnt/local-storage/hdd",
        "mountDir": "/mnt/local-storage/hdd"
      }
----
<1> Name of the StorageClass.
<2> Path to the directory on the host. It must be a subdirectory of *_/mnt/local-storage_*.
<3> Path to the directory in the provisioner pod. We recommend using the same directory structure as used on the host.

With this configuration, the provisioner creates:

* One PV with StorageClass `local-ssd` for every subdirectory in *_/mnt/local-storage/ssd_*.
* One PV with StorageClass `local-hdd` for every subdirectory in *_/mnt/local-storage/hdd_*.

[[local-volume-deployment-local-provisioner]]
=== Deploy Local Provisioner

[NOTE]
====
Before starting the provisioner, mount all local devices and create a ConfigMap
with storage classes and their directories.
====

Install the local provisioner from the link:https://raw.githubusercontent.com/jsafrane/origin/local-storage/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml[*_local-storage-provisioner-template.yaml_*] file.

. Create a service account that allows running pods as a root user and use hostPath volumes:
+
[source, bash]
----
$ oc create serviceaccount local-storage-admin
$ oc adm policy add-scc-to-user privileged -z local-storage-admin
----

To allow the provisioner pod to delete content on local volumes created by any
pod, root privileges and any SELinux context are required. hostPath is required
to access the *_/mnt/local-storage_* path on the host.

. Install the template:
+
[source, bash]
----
$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml
----

. Instantiate the template by specifying values for `configmap`, `account`, and `provisioner_image` parameters:
+
[source, bash]
----
$ oc new-app -p CONFIGMAP=local-volume-config \
  -p SERVICE_ACCOUNT=local-storage-admin \
  -p NAMESPACE=local-storage local-storage-provisioner
----

See the link:https://raw.githubusercontent.com/jsafrane/origin/local-storage/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml[template] for other configurable options. This template creates a DaemonSet that runs a
pod on every node. The pod watches directories specified in the ConfigMap and
automatically creates PVs for them.

The provisioner runs as root to clean up the directories when a PV is released and all data needs to be removed.

[[local-volume-adding-new-devices]]
=== Adding New Devices
Adding a new device requires several manual steps:

. Stop DaemonSet with the provisioner.
. Create a subdirectory in the right directory on the node with the new device and mount it there.
. Start the DaemonSet with the provisioner.

[IMPORTANT]
====
Omitting any of these steps may result in the wrong PV being created.
====